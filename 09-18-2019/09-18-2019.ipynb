{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives:\n",
    "\n",
    "In this lecture, we will learn the following concepts about decision tree classifier:\n",
    "1. Motivation.\n",
    "2. Divide and conquer and greedy approaches.\n",
    "3. Entropy and information gain-- their roles to select the best attribute.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revision:\n",
    "\n",
    "1. Concepts of each classifier that we learned.\n",
    "2. Pros and cons of each classifier that we learned-- Discussion for the next lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Decision tree classifier][1]\n",
    "\n",
    "\n",
    "#### Most of this lecture are taken from or spired by the [pdf][1] and other links. I uploaded the pdf in my GitHub folder.\n",
    "\n",
    "\n",
    "## [Motivation][2]\n",
    "\n",
    "\n",
    "Suppose we have two classes represented by black circle and blue squares:\n",
    "\n",
    "\n",
    "\n",
    "<img style=\"float:center\" src=\"./images/Splitting_Space.png\" alt=\"drawing\" height=\"200\" width=\"300\"/>\n",
    "\n",
    "We will need more than one line, to divide into classes, such as:\n",
    "\n",
    "<img style=\"float:center\" src=\"./images/Splitting_Space_MoreLines.png\" alt=\"drawing\" height=\"200\" width=\"300\"/>\n",
    "\n",
    "\n",
    "We need two lines here one separating according to threshold value of x and other for threshold value of y.\n",
    "\n",
    "<b> So when does it terminate: </b>\n",
    "\n",
    "\n",
    "1. Either, it has divided into classes that are pure (only containing members of single class ). \n",
    "2. Or, Some criteria of classifier attributes are met.\n",
    "\n",
    "### Decision tree definition:\n",
    "\n",
    "<b> Decision Tree Classifier </b>, repetitively divides <b> (in a divide and conquare manner) </b> the working area(plot) into sub part by identifying lines. (repetitively because there may be two distant regions of same class divided by other as shown in image below).\n",
    "\n",
    "\n",
    "### Decision tree components:\n",
    "\n",
    "\n",
    "\n",
    "Given our example:\n",
    "\n",
    "<img style=\"float:center\" src=\"./images/Example-Semi.png\" alt=\"drawing\" height=\"400\" width=\"600\"/>\n",
    "\n",
    "\n",
    "Sub-Tree like the below:\n",
    "\n",
    "<img style=\"float:center\" src=\"./images/DT-Example.png\" alt=\"drawing\" height=\"300\" width=\"500\"/>\n",
    "\n",
    "\n",
    "<b> Components </b>\n",
    "\n",
    "1. Root node: The topmost. (Here is the age?)\n",
    "2. Branches: Represent the outcomes of tests. (e.g. here we have three branches from age?)\n",
    "3. Leaf nodes: Terminal nodes that holds class labels. (Here we have 4 nodes)\n",
    "4. Internal nodes: Non leaves or root nodes that need further splitting?\n",
    "\n",
    "\n",
    "### Typical output from the sub decision tree:\n",
    "\n",
    "Typical output is a rule-based system that summarizes the DT branches:\n",
    "\n",
    "\n",
    "if (age == youth) and (student=no) then Class = no <br/>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else if (age == youth) and (student=yes) then Class = yes <br/>\n",
    "else if (age == Middle-aged) then Class = yes <br/>\n",
    "else if (age=senior) and (credit-rating=fair) then Class = no <br/>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (age == senior) and (credit-rating=Excellent) then Class = yes <br/>\n",
    "\n",
    "\n",
    "\n",
    "### Here we have two decisions:\n",
    "\n",
    "\n",
    "\n",
    "1. What is the best attribute or feature to split?\n",
    "2. When do we have to stop?\n",
    "\n",
    "\n",
    "\n",
    "### [Types of decision trees][3]\n",
    "\n",
    "The approaches of decision tree algorithm is to select the best attribute that gives most information (greedy search). \n",
    "\n",
    "\n",
    "[1]:https://www.inf.unibz.it/~mkacimi/lecture5.pdf\n",
    "[2]:https://medium.com/machine-learning-101/chapter-3-decision-trees-theory-e7398adac567\n",
    "[3]: https://www.enplusadvisors.com/decision-trees/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "\n",
    "<b> Given our example: </b>\n",
    "\n",
    "<img style=\"float:center\" src=\"./images/Example-Semi.png\" alt=\"drawing\" height=\"400\" width=\"600\"/>\n",
    "\n",
    "<b> Let us select: <u> age</u> </b>\n",
    "\n",
    "\n",
    "<img style=\"float:center\" src=\"./images/Age-Example.png\" alt=\"drawing\" height=\"400\" width=\"600\"/>\n",
    "\n",
    "<img style=\"float:center\" src=\"./images/Age-Splittting-Middle-aged.png\" alt=\"drawing\" height=\"400\" width=\"600\"/>\n",
    "\n",
    "\n",
    "\n",
    "<b> Let us select: <u> student</u> </b> based on our selection for <b> age </b>\n",
    "\n",
    "<img style=\"float:center\" src=\"./images/Student-Example.png\" alt=\"drawing\" height=\"400\" width=\"600\"/>\n",
    "\n",
    "<img style=\"float:center\" src=\"./images/Student-Example-Majority.png\" alt=\"drawing\" height=\"400\" width=\"600\"/>\n",
    "\n",
    "<img style=\"float:center\" src=\"./images/Student-Example-Selection.png\" alt=\"drawing\" height=\"400\" width=\"600\"/>\n",
    "\n",
    "\n",
    "<b> And so on</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The three possible partition senarios:\n",
    "\n",
    "<img style=\"float:center\" src=\"./images/Partitioning Scenarios.png\" alt=\"drawing\" height=\"400\" width=\"600\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribute selection measures:\n",
    "\n",
    "The measures are heurstic methods for selecting the splitting criterion that best seperates a given data partition D. We aim to have a pure partition with all tuples or data points belonging to the same class.\n",
    "\n",
    "<b> Attribute selection measures (splitting rules) </b>\n",
    "1. Determine how the tuples at a given node are to be split.\n",
    "2. Provide ranking for each attribute describing the tuples.\n",
    "3. The attribute with highest score is chosen.\n",
    "4. Determine a split point or a splitting subset.\n",
    "\n",
    "## Methods\n",
    "1. Information gain\n",
    "2. Gain ratio\n",
    "3. Gini Index \n",
    "\n",
    "<b> All these methods are based on the concept of </b> [Entropy][1]\n",
    "\n",
    "## Entropy\n",
    "\n",
    "The Entropy is a measure of the average information content one is missing when one does not know the value of the random variable. <br>\n",
    "\n",
    "\n",
    "### Now the math definition of entropy\n",
    "\n",
    "Given that $X$ takes $n$ values $V_{1},V_{2},\\cdots,V_{n}$ and \n",
    "\n",
    "<center> $P(X=V_{1})=p_{1}, P(X=V_{2})=p_{2},\\cdots, P(X=V_{n})=p_{n}$ </center>\n",
    "\n",
    "The entropy is the smallest number of bits $(H(x))$, on average, per symbol, needed to transmit the symbols drawn from distribution of X is given by: <br/> <br/>\n",
    "<center>$ H(X) = - \\sum_{i}^n p_{i}\\log_{2}(p_{i})$  </center>\n",
    "\n",
    "[<b>Example</b>][2]\n",
    "\n",
    "1. Consider we have X with 2 equal possibilties $P(X=A)=P(X=B)\\frac{1}{2}$. So, $H(X)=-0.5\\log_{2}0.5-0.5\\log_{2}0.5=1$\n",
    "2. Consider we have X with 2 unequal possibilties $P(X=A)=0.36$ and $P(X=B)=0.64$. So,  $H(X)=-0.36\\log_{2}0.36-0.64\\log_{2}0.64 =0.94$\n",
    "\n",
    "\n",
    "<b> High Entropy </b>\n",
    "1. X is from a uniform like distribution.\n",
    "2. Flat histogram.\n",
    "3. Values sampled from it are less predictable.\n",
    "\n",
    "<b> Low Entropy </b>\n",
    "\n",
    "1. X is from a varied (peaks and valleys) distribution\n",
    "2. Histogram has many lows and highs\n",
    "3. Values sampled from it are more predictable \n",
    "\n",
    "[1]:https://study.com/academy/lesson/what-is-entropy-definition-law-formula.html\n",
    "[2]:http://saedsayad.com/decision_tree.htm\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Gain\n",
    "\n",
    "It is the approach that minimizes the expected number of tests needed to classify a given tuple and guarantee a simple tree. \n",
    "\n",
    "Assume:\n",
    "1. D: the current partition\n",
    "2. N: represent the tuples of partition D\n",
    "<br/><br/>\n",
    "\n",
    "Select the attribute with the highest information gain (based on the work by Shannon on information theory). This attribute:\n",
    "1. minimizes the information needed to classify the tuples in the resulting partitions\n",
    "2. reflects the least randomness or “impurity” in these partitions \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [The first step of information gain][1]\n",
    "\n",
    "\n",
    "\n",
    "To calculate the expected information or the entropy needed to classify a data point from all data points. <br/><br/>\n",
    "<center> Info$(D) = -\\sum_{i}^mp_{i}\\log_{2}(p_{i}) $ </center>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<img style=\"float:center\" src=\"./images/First-Step.png\" alt=\"drawing\" height=\"500\" width=\"700\"/>\n",
    "\n",
    "[1]:https://www.inf.unibz.it/~mkacimi/lecture5.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [The second step of information gain][1]\n",
    "\n",
    "\n",
    "\n",
    "For each attribute, compute the amount of information needed to arrive at an exact classification after portioning using that attribute: <br/><br/>\n",
    "<center> Info$_{A}(D) = -\\sum_{j}^v\\frac{\\lvert D_{j}\\rvert}{\\lvert D\\rvert} $info $(D_{j})$ </center>\n",
    "\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "<img style=\"float:center\" src=\"./images/Second-Step.png\" alt=\"drawing\" height=\"500\" width=\"700\"/>\n",
    "\n",
    "\n",
    "\n",
    "#### Of note: \n",
    "    - for each value, you need to apply the first step with total of data points equal that value data points distributed with its class label points.\n",
    "\n",
    "\n",
    "\n",
    "[1]:https://www.inf.unibz.it/~mkacimi/lecture5.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [The third step of information gain][1]\n",
    "\n",
    "\n",
    "\n",
    "- Compute Infrmation Gain by branching on each attribute or feature A is:\n",
    "\n",
    "\n",
    "<center> Gain$(A) =$ Info $(D) - $ info$_{A}(D)$ </center>\n",
    "\n",
    "- Information gain is the expected reduction in the information requirements caused by knowing the value of A.\n",
    "\n",
    "-  The attribute A with the highest information gain (Gain(A)), is chosen as the splitting attribute at node N \n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "<img style=\"float:center\" src=\"./images/Third-Step.png\" alt=\"drawing\" height=\"500\" width=\"700\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[1]:https://www.inf.unibz.it/~mkacimi/lecture5.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
